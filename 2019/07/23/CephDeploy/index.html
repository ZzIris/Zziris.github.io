<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Ceph,分布式存储,">










<meta name="description" content="一. 前言&amp;emsp;&amp;emsp;由于近期开始接触学习Ceph分布式存储技术，且搭建了Ceph集群，本文将对个人搭建Ceph集群这一过程步骤进行梳理，并对这一过程中遇到的问题进行总结且给出解决方法。 二. Ceph集群搭建操作步骤&amp;emsp;&amp;emsp;首先我们的Ceph集群搭建是在虚拟机上进行的，使用系统为CentOs 7.5，总共开了四台虚拟机，一台作为部署机，其余三台作为集群的节点，其IP地">
<meta name="keywords" content="Ceph,分布式存储">
<meta property="og:type" content="article">
<meta property="og:title" content="Ceph集群搭建教程及一些学习搭建过程中遇到的相关操作与问题">
<meta property="og:url" content="http://ZzIris.github.io/2019/07/23/CephDeploy/index.html">
<meta property="og:site_name" content="Ching">
<meta property="og:description" content="一. 前言&amp;emsp;&amp;emsp;由于近期开始接触学习Ceph分布式存储技术，且搭建了Ceph集群，本文将对个人搭建Ceph集群这一过程步骤进行梳理，并对这一过程中遇到的问题进行总结且给出解决方法。 二. Ceph集群搭建操作步骤&amp;emsp;&amp;emsp;首先我们的Ceph集群搭建是在虚拟机上进行的，使用系统为CentOs 7.5，总共开了四台虚拟机，一台作为部署机，其余三台作为集群的节点，其IP地">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2021-05-20T14:51:05.157Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ceph集群搭建教程及一些学习搭建过程中遇到的相关操作与问题">
<meta name="twitter:description" content="一. 前言&amp;emsp;&amp;emsp;由于近期开始接触学习Ceph分布式存储技术，且搭建了Ceph集群，本文将对个人搭建Ceph集群这一过程步骤进行梳理，并对这一过程中遇到的问题进行总结且给出解决方法。 二. Ceph集群搭建操作步骤&amp;emsp;&amp;emsp;首先我们的Ceph集群搭建是在虚拟机上进行的，使用系统为CentOs 7.5，总共开了四台虚拟机，一台作为部署机，其余三台作为集群的节点，其IP地">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://ZzIris.github.io/2019/07/23/CephDeploy/">





  <title>Ceph集群搭建教程及一些学习搭建过程中遇到的相关操作与问题 | Ching</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3ef8bbb3049eedc7533c61fb79f60f5f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ching</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://ZzIris.github.io/2019/07/23/CephDeploy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ching">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/IMG_2745.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ching">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Ceph集群搭建教程及一些学习搭建过程中遇到的相关操作与问题</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-23T21:19:14+08:00">
                2019-07-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Ceph/" itemprop="url" rel="index">
                    <span itemprop="name">Ceph</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="一-前言"><a href="#一-前言" class="headerlink" title="一. 前言"></a>一. 前言</h1><p>&emsp;&emsp;由于近期开始接触学习Ceph分布式存储技术，且搭建了Ceph集群，本文将对个人搭建Ceph集群这一过程步骤进行梳理，并对这一过程中遇到的问题进行总结且给出解决方法。</p>
<h1 id="二-Ceph集群搭建操作步骤"><a href="#二-Ceph集群搭建操作步骤" class="headerlink" title="二. Ceph集群搭建操作步骤"></a>二. Ceph集群搭建操作步骤</h1><p>&emsp;&emsp;首先我们的Ceph集群搭建是在虚拟机上进行的，使用系统为CentOs 7.5，总共开了四台虚拟机，一台作为部署机，其余三台作为集群的节点，其IP地址如下所示：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy 192.168.0.223</span><br><span class="line">node1 192.168.0.105</span><br><span class="line">node2 192.168.0.106</span><br><span class="line">node3 192.168.0.107</span><br></pre></td></tr></table></figure></p>
<h2 id="1-前期准备工作"><a href="#1-前期准备工作" class="headerlink" title="1. 前期准备工作"></a>1. 前期准备工作</h2><h3 id="安装epel-release"><a href="#安装epel-release" class="headerlink" title="安装epel-release"></a>安装epel-release</h3><p>&emsp;&emsp;由于RHEL以及其衍生发行版如CentOs等Linux为了稳定，其官方的rpm repository提供的rpm包往往很滞后，当然这是理所应当的，毕竟是服务器版本，服务器的安全稳定才是首要的。而epel的出现就是为了解决官方rpm仓库内容不够丰富且版本滞后的问题，epel全称为Extra Packages for Enterprise Linux。是由Fedora社区打造的，为RHEL及其衍生发行版本提供高质量软件包的项目。装了epel就相当于添加了一个第三方源。
&emsp;&emsp;我们可以通过以下命令来对epel-release来进行安装：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br></pre></td></tr></table></figure></p>
<h3 id="安装Ceph相关的包"><a href="#安装Ceph相关的包" class="headerlink" title="安装Ceph相关的包"></a>安装Ceph相关的包</h3><p>&emsp;&emsp;首先我们需要在部署机上安装ceph-deploy部署工具，以在后续通过该工具对其余节点进行自动化部署。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ceph-deploy -y</span><br></pre></td></tr></table></figure></p>
<h3 id="安装openssh-server"><a href="#安装openssh-server" class="headerlink" title="安装openssh-server"></a>安装openssh-server</h3><p>&emsp;&emsp;在部署过程中我们需要通过部署机使用ssh连接到各个节点上进行操作，所以需要下载ssh服务，而在我在最小化安装了CentOs7.5后该服务都是已经装好的，无需再装，若发现没有装的话再进行安装的命令。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install openssh-server -y</span><br></pre></td></tr></table></figure></p>
<h3 id="为部署机及每个集群节点机创建用户并进行sudo免密操作"><a href="#为部署机及每个集群节点机创建用户并进行sudo免密操作" class="headerlink" title="为部署机及每个集群节点机创建用户并进行sudo免密操作"></a>为部署机及每个集群节点机创建用户并进行sudo免密操作</h3><p>&emsp;&emsp;在这里比较推荐在部署机以及集群节点机上新建用于集群工作的用户，官方文档申明在新建这些用户时不要使用”ceph”来作为用户名，也最好不要使用root用户来进行操作，总之，新建一个新的用户用于创建集群是最好的。而在我的搭建过程中部署机用户名设为deploy，其余三个集群节点用户名分别为node1、node2、node3。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo useradd -d /home/&#123;username&#125; -m &#123;username&#125;   #username为你需要创建的用户名</span><br><span class="line">$ sudo passwd &#123;username&#125;   #设置用户密码</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;对每个节点进行sudo的免密设置，由于部署机在使用命令对所有节点进行部署的时候都是自动进行的，期间其会自动ssh连接到相应的节点机上进行sudo的操作来写入一些配置文件并且不会提示用户进行密码输入，所以需要提前设置好sudo的免密，以让部署机能够顺利对节点进行部署。（我自己将部署机也同时设置了sudo免密）。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo echo &quot;&#123;username&#125; ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/&#123;username&#125;</span><br><span class="line">$ sudo chmod 0440 /etc/sudoers.d/&#123;username&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="设置部署机与其余集群节点的免密ssh连接"><a href="#设置部署机与其余集群节点的免密ssh连接" class="headerlink" title="设置部署机与其余集群节点的免密ssh连接"></a>设置部署机与其余集群节点的免密ssh连接</h3><p>&emsp;&emsp;通过部署机对其余集群节点进行部署需要让部署机与其余节点进行连接，因此需要用到ssh连接，我们需要在部署机上生成ssh密钥,输入命令后一直回车就好。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen</span><br></pre></td></tr></table></figure></p>
<p>在执行命令后，将在我们的当前部署工作所在的用户家目录下的.ssh文件(其为隐藏文件)下生成一个id_rsa和id_rsa.pub两个文件，前者记录着私钥，后者记录着公钥，我们接下来需要将公钥拷贝到我们需要建立的集群节点机上。
&emsp;&emsp;此处我将我的deploy节点上的公钥分别拷贝到node1,node2和node3上,于是在deploy上执行：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id node1@192.168.0.105</span><br><span class="line">$ ssh-copy-id node2@192.168.0.106</span><br><span class="line">$ ssh-copy-id node3@192.168.0.107</span><br></pre></td></tr></table></figure></p>
<p>这样便将公钥分别拷贝到三个集群节点上了。
&emsp;&emsp;由于我们进行我们进行进行ssh连接时一般需要输入{username}@{hostip}的形式，为了简化ssh连接的形式让ceph-deploy工具顺利部署，我们需要修改一些配置文件，修改如下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/hosts</span><br></pre></td></tr></table></figure></p>
<p>在该文件中增加一些内容，将ip换成一个对应名字：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.0.105 cpnode1</span><br><span class="line">192.168.0.106 cpnode2</span><br><span class="line">192.168.0.107 cpnode3</span><br></pre></td></tr></table></figure></p>
<p>再修改.ssh文件夹下的config，若不存在该文件则创建它：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim ~/.ssh/config</span><br></pre></td></tr></table></figure></p>
<p>增加内容为：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Host ceph1</span><br><span class="line">   Hostname cpnode1</span><br><span class="line">   User node1</span><br><span class="line">Host ceph2</span><br><span class="line">   Hostname cpnode2</span><br><span class="line">   User node2</span><br><span class="line">Host ceph3</span><br><span class="line">   Hostname cpnode3</span><br><span class="line">   User node3</span><br></pre></td></tr></table></figure></p>
<p>修改完以上文件后，你就可以直接以如下形式进行ssh连接且需要再输入用户密码，方便了许多。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh ceph1</span><br><span class="line">$ ssh ceph2</span><br><span class="line">$ ssh ceph3</span><br></pre></td></tr></table></figure></p>
<h3 id="关闭firewall服务和selinux服务"><a href="#关闭firewall服务和selinux服务" class="headerlink" title="关闭firewall服务和selinux服务"></a>关闭firewall服务和selinux服务</h3><p>&emsp;&emsp;由于firewall和selinux会在部署过程中造成一些影响，所以建议直接关闭。
&emsp;&emsp;关闭firewall：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl disable firewalld</span><br><span class="line">$ sudo systemctl stop firewalld</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;关闭selinux：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure></p>
<p>修改文件内容如下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></p>
<p>然后执行以下命令：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo setenforce 0</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这样便可以关闭firewall和selinux了。</p>
<h3 id="安装Ceph"><a href="#安装Ceph" class="headerlink" title="安装Ceph"></a>安装Ceph</h3><p>&emsp;&emsp;在这里我们打算使用13.2.5版本的Ceph，由于在写这篇文章时mimic大版本的ceph最新已经是13.2.6，于是这里就涉及到了指定版本的安装。在这里我们介绍两种安装方式：1.直接通过外网镜像源安装。2.通过自己搭建本地源进行安装。（注：再官方给出的文档中是通过ceph-deploy install来进行自动安装ceph，但该种安装方式存在多种弊端，如指定版本或指定下载源比较麻烦等，因此比较推荐在每个集群节点上直接使用yum来安装。）</p>
<h4 id="方式一：通过外网镜像源安装"><a href="#方式一：通过外网镜像源安装" class="headerlink" title="方式一：通过外网镜像源安装"></a>方式一：通过外网镜像源安装</h4><p>&emsp;&emsp;这种方式的优点是操作步骤较为简单，但缺点是受外网镜像源的影响，安装速度慢且有时候会发生寻找不到有效源。该方式只需要在集群机上进行，无需在部署机上进行。
&emsp;&emsp;使用该方式进行安装，我们首先需要在本地的yum源仓库文件中增添Ceph的镜像源仓库文件，操作如下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ touch /etc/yum.repo.d/ceph.repo</span><br><span class="line">$ sudo vim /etc/yum.repo.d/ceph.repo</span><br></pre></td></tr></table></figure></p>
<p>我们在ceph.repo中增添如下内容：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure></p>
<p>可以看到，在该增添的内容中，我们使用的是阿里云的源，而没有使用官方源，这是因为国内的源下载速度会比国外的源快一些。
&emsp;&emsp;当我们添加完Ceph有关的repo文件后，就可以进行安装了，命令如下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ceph-13.2.5 ceph-radosgw-13.2.5 -y</span><br></pre></td></tr></table></figure></p>
<p>这里注意，想要指定相应版本，需要连同ceph-radosgw一起指定安装，因为ceph-radosgw中会有ceph的依赖项，单独指定ceph-13.2.5是无法完成ceph rpm包的下载安装的。</p>
<h4 id="方式二：通过自己搭建本地源进行安装"><a href="#方式二：通过自己搭建本地源进行安装" class="headerlink" title="方式二：通过自己搭建本地源进行安装"></a>方式二：通过自己搭建本地源进行安装</h4><p>&emsp;&emsp;为了更快地进行安装，我们可以先将ceph相关的rpm包下载到部署机上后，建立本地源仓库来进行安装，这种安装方式在安装速度上是绝对占优的且不受外网镜像源的影响，但前提是需要现把rpm包下载下来。比较推荐学会搭建本地源，因为在实际工作环境中，很多机房中服务器是无法进行直接通过yum外网镜像源安装的，所以需要搭建本地源来解决。</p>
<p>首先我们先下载安装一个创建yum源仓库的工具createrepo：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install createrepo -y</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;由于需要在部署机上将ceph相关的rpm包下载下来，我们需要在部署机上的/etc/yum.repo.d/路径下添加ceph相关的.repo文件，步骤在方式一中已给出不再赘述。当.repo文件添加完后就进行下载，这里注意，我只需要将rpm包下载下来即可，不需要安装，因此命令为：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install --downloadonly --downloaddir=/mnt/myrepo/ceph/ ceph-13.2.5 ceph-radosgw-13.2.5</span><br></pre></td></tr></table></figure></p>
<p>此处我将相关的包下载到/mnt/myrepo/ceph/路径下，这个路径根据自己喜好来进行即可。
&emsp;&emsp;下载完毕后，我们将myrepo文件夹建立成仓库：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo createrepo /mnt/myrepo/</span><br></pre></td></tr></table></figure></p>
<p>命令执行结束后，便会在myrepo文件夹下生成repodata文件夹，此时代表仓库建立成功，此时通过createrepo后，仓库里的各个包之间的依赖关系就建立起来了，这样就可以通过yum命令来进行安装下载下来的包而不需要考虑rpm包之间的依赖关系了。
&emsp;&emsp;由于我们的ceph仓库是建立在部署机上的，为了能让其他集群节点能访问到且进行安装，我们需要在部署机上打开http服务，将仓库文件在局域网内共享。首先我们需下载http相关的包并进行安装：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install httpd -y</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;然后打开httpd且设为开机启动，然后查看其是否已打开。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl enable httpd</span><br><span class="line">$ sudo systemctl start httpd</span><br><span class="line">$ systemctl status httpd   #状态为active即为打开</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;打开服务后，我们将我们创建好的仓库文件夹做一个软连接映射到httpd的默认共享路径下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ln -s /mnt/myrepo /var/www/html/</span><br></pre></td></tr></table></figure></p>
<p>或者也可以不做软链接，直接到httpd的配置文件中去修改其默认的共享文件夹为我们的仓库目录：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/httpd/conf/httpd.conf</span><br></pre></td></tr></table></figure></p>
<p>修改内容为：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">DocumentRoot &quot;/mnt/myrepo&quot;</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Relax access to content within /var/www.</span><br><span class="line">#</span><br><span class="line">&lt;Directory &quot;/mnt/myrepo&quot;&gt;</span><br><span class="line">    AllowOverride None</span><br><span class="line">    # Allow open access:</span><br><span class="line">    Require all granted</span><br><span class="line">&lt;/Directory&gt;</span><br><span class="line"></span><br><span class="line"># Further relax access to the default document root:</span><br><span class="line">&lt;Directory &quot;/mnt/myrepo&quot;&gt;</span><br><span class="line">    #</span><br><span class="line">    # Possible values for the Options directive are &quot;None&quot;, &quot;All&quot;,</span><br><span class="line">    # or any combination of:</span><br><span class="line">    #   Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews</span><br><span class="line">    #</span><br><span class="line">    # Note that &quot;MultiViews&quot; must be named *explicitly* --- &quot;Options All&quot;</span><br><span class="line">    # doesn&apos;t give it to you.</span><br><span class="line">    #</span><br><span class="line">    # The Options directive is both complicated and important.  Please see</span><br><span class="line">    # http://httpd.apache.org/docs/2.4/mod/core.html#options</span><br><span class="line">    # for more information.</span><br><span class="line">    #</span><br><span class="line">    #Options Indexes FollowSymLinks</span><br><span class="line">     Options ALL</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #</span><br><span class="line">    # AllowOverride controls what directives may be placed in .htaccess files.</span><br><span class="line">    # It can be &quot;All&quot;, &quot;None&quot;, or any combination of the keywords:</span><br><span class="line">    #   Options FileInfo AuthConfig Limit</span><br><span class="line">    #</span><br><span class="line">    AllowOverride None</span><br><span class="line"></span><br><span class="line">    #</span><br><span class="line">    # Controls who can get stuff from this server.</span><br><span class="line">    #</span><br><span class="line">    Require all granted</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改几处与路径相关的地方，同时将Option后面改为ALL。然后我们再到将httpd的一个欢迎页面配置文件给删除掉，这里为了防止文件丢失，我们进行改名让其无法生效即可：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mv /etc/httpd/conf.d/welcome.conf /etc/httpd/conf.d/welcome.conf.bak</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;最后我们重启一下httpd服务即可：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart network</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;接下来我们来验证在部署机上搭建的本地源是否已将文件共享出来，我们打开浏览器在地址栏输入以下内容（192.168.0.223为我建立本地仓库的部署机的IP，此处应相应换成你们所搭建源的主机IP，下同）：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.0.223/myrepo   #若是做了软连接则输入该形式</span><br><span class="line">http://192.168.0.223   #若是修改了httpd的配置文件的默认共享地址则输入该形式</span><br></pre></td></tr></table></figure></p>
<p>如果能在浏览器中看到我们的仓库文件内容即表明仓库文件共享成功。
&emsp;&emsp;由于使用的是本地搭建的源而不是外网源，所以本方式集群节点机上的ceph相关的.repo文件应该这么修改：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://192.168.0.223/myrepo</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=//192.168.0.223/myrepo</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=//192.168.0.223/myrepo</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure></p>
<p>由于是本地源，我们可以选择把gpgcheck给关掉。
&emsp;&emsp;为了避免其他源对我们所建的本地源的影响，我们可以将其他的源移到别的路径下备份起来：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir /etc/yum.repo.d/backup</span><br><span class="line">$ sudo mv /etc/yum.repo.d/*.repo /etc/yum.repo.d/backup   #此时ceph.repo也被移进了backup</span><br><span class="line">$ sudo mv /etc/yum.repo.d/backup/ceph.repo /etc/yum.repo.d/   #将已创建的ceph.repo移回原位</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;然后直接进行yum安装指令即可，注意此时由于我们下载下来搭建本地源的ceph已经是13.2.5版本了，我们无需再进行版本指定。因为仓库里只有13.2.5版本的ceph。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ceph ceph-radosgw -y</span><br></pre></td></tr></table></figure></p>
<p>注意：如果在httpd共享文件后，别的主机无法访问到你所搭建的仓库，则有可能是firewall服务或者selinux服务没关，导致搭建源的主机拒绝任何访问从而无法共享文件，此时应查看相应的两个服务是否关掉，如果这两个服务都关掉了还是无法访问则需查看搭建源的每一级目录是否有执行权限，如果没有执行权限也会导致无法访问路径下的内容。</p>
<p>&emsp;&emsp;通过以上步骤，我们就完成了搭建Ceph集群的准备工作，接下来便进入正式部署集群阶段。</p>
<h2 id="2-部署Ceph集群"><a href="#2-部署Ceph集群" class="headerlink" title="2. 部署Ceph集群"></a>2. 部署Ceph集群</h2><p>&emsp;&emsp;此部分的部署工作都在部署机上进行，涉及到ceph-deploy这一命令的都是在部署机上执行的，在该实验中，我的部署机上没有装ceph，单纯只为用于部署工作，当然也可以将部署机作为一个集群节点。</p>
<h3 id="新建部署所用目录"><a href="#新建部署所用目录" class="headerlink" title="新建部署所用目录"></a>新建部署所用目录</h3><p>&emsp;&emsp;首先在进行部署集群之前，我们在部署机的部署用户下创建一个用于存放部署文件的目录（目录名称任取）：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir ~/my-cluster</span><br><span class="line">$ cd ~/my-cluster</span><br></pre></td></tr></table></figure></p>
<h3 id="创建monitor"><a href="#创建monitor" class="headerlink" title="创建monitor"></a>创建monitor</h3><p>&emsp;&emsp;首先创建集群，初始化将要指定为monitor的节点,此处我先将node1和node2指定为初始化的monitor节点：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy new node1 node2</span><br></pre></td></tr></table></figure></p>
<p>在执行完命令之后，你之前在部署机上所创建的所用于部署的目录下将新增以下几个文件：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph.conf  </span><br><span class="line">ceph-deploy-ceph.log  </span><br><span class="line">ceph.mon.keyring</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;接下来，我们需要创建前面已初始化好的monitor节点：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure></p>
<p>则所用于部署的目录下又再新增了几个文件：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph.bootstrap-mds.keyring                 </span><br><span class="line">ceph.client.admin.keyring</span><br><span class="line">ceph.bootstrap-mgr.keyring                 </span><br><span class="line">ceph.bootstrap-osd.keyring </span><br><span class="line">ceph.bootstrap-rgw.keyring</span><br></pre></td></tr></table></figure></p>
<h3 id="分发命令权限"><a href="#分发命令权限" class="headerlink" title="分发命令权限"></a>分发命令权限</h3><p>&emsp;&emsp;然后我们将为我们想要其能执行ceph命令的节点分发admin.keyring:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy admin node1 node2</span><br></pre></td></tr></table></figure></p>
<p>没有被分发admin.keyring的节点是无法使用ceph命令的，而这个admin.keyring的文件将出现再被分发节点的/etc/ceph目录下，再该目录下，还会看到与部署机一致的ceph.conf文件，在整个集群配置时，该文件都要保持一致，一旦部署机上的conf文件修改，都要推送覆盖到其余节点上，这样才能保证配置不会出错。
&emsp;&emsp;在该命令执行完后，我们可以在任意一台本分发了admin.keyring的节点机输入以下命令测试一下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph -s</span><br></pre></td></tr></table></figure></p>
<p>该命令可以查看集群的整体情况。</p>
<h3 id="创建mgr"><a href="#创建mgr" class="headerlink" title="创建mgr"></a>创建mgr</h3><p>&emsp;&emsp;接下来再创建mgr:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mgr create node1 node2</span><br></pre></td></tr></table></figure></p>
<p>mgr可以在两三个节点上创建，但也没必要太多。</p>
<h3 id="创建osd"><a href="#创建osd" class="headerlink" title="创建osd"></a>创建osd</h3><p>&emsp;&emsp;最后我们便可以创建osd了，在这里需注意，创建使用lvm分区的osd和创建使用块设备的osd命令略有不同，而我的机子上使用的是lvm分区，所以命令如下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy osd create --data vg1/lv1 node1</span><br><span class="line">$ ceph-deploy osd create --data vg2/lv2 node2</span><br></pre></td></tr></table></figure></p>
<p>如果需要创建使用块设备的osd，则需要输入：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy osd create --data /dev/sdb node1</span><br><span class="line">$ ceph-deploy osd create --data /dev/sdc node2</span><br></pre></td></tr></table></figure></p>
<p>可以看出，这两种方式在命令上不同的地方为—data后的形式不一样，在使用lvm分区的情况，格式为 卷组名/逻辑卷名，卷组名和逻辑卷名是在创建lvm分区时就已确定。</p>
<h3 id="查看集群部署情况"><a href="#查看集群部署情况" class="headerlink" title="查看集群部署情况"></a>查看集群部署情况</h3><p>&emsp;&emsp;在执行完创建osd的命令后，Ceph集群的几个基本组件就都部署好了，我们可以在集群节点上通过以下命令来查看当前的集群情况：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph health</span><br></pre></td></tr></table></figure></p>
<p>如以下命令执行完后出现：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HEALTH_OK</span><br></pre></td></tr></table></figure></p>
<p>则表明集群状况良好。除此之外，我们还可以通过另一指令来查看更详细的集群情况：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph -s</span><br></pre></td></tr></table></figure></p>
<p>命令执行后将得到以下信息：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cluster:</span><br><span class="line">    id:     85313a2d-104f-4425-a033-7f17888f8021</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 2 daemons, quorum node3,node2</span><br><span class="line">    mgr: node2(active), standbys: node3</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   2.0 GiB used, 28 GiB / 30 GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure></p>
<p>在这里面我们将可以看到monitor的守护进程已存在，mgr已激活，osd也成功创建且状态都处于up和in，集群整体情况很健康。</p>
<h2 id="3-新增monitor节点："><a href="#3-新增monitor节点：" class="headerlink" title="3. 新增monitor节点："></a>3. 新增monitor节点：</h2><p>&emsp;&emsp;通过上述步骤我们知道，创建Ceph集群的第一步就是初始化需要指定为monitor的节点，而这里的新增monitor节点指的是没有经过第一步new过的节点，此时我们需要先修改ceph.conf文件，我们在部署机上进行修改，并把修改后的ceph.conf文件推送覆盖其他节点的ceph.conf文件，使它们的这个设置文件保持一致：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim ~/ceph.conf</span><br></pre></td></tr></table></figure></p>
<p>修改的文件内容为：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">fsid = 85313a2d-104f-4425-a033-7f17888f8021</span><br><span class="line">mon_initial_members = node1, node2</span><br><span class="line">mon_host = 192.168.0.105,192.168.0.106</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line"></span><br><span class="line">public_network=192.168.0.0/24</span><br></pre></td></tr></table></figure></p>
<p>此处public_network的设置要使得你所增加的monitor节点和已存在的节点的IP同处一个网段。
然后增加你想要增加的monitor节点，此处我们将node3新增为monitor节点：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mon add node3</span><br></pre></td></tr></table></figure></p>
<p>这样一个新的monitor节点就添加完成了。</p>
<h2 id="4-新增osd节点："><a href="#4-新增osd节点：" class="headerlink" title="4. 新增osd节点："></a>4. 新增osd节点：</h2><p>&emsp;&emsp;新增osd节点比较简单，直接创建即可：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy osd create --data  vg3/lv1 node3</span><br></pre></td></tr></table></figure></p>
<h2 id="5-删除osd节点："><a href="#5-删除osd节点：" class="headerlink" title="5. 删除osd节点："></a>5. 删除osd节点：</h2><p>&emsp;&emsp;首先我们先明确需要剔除的osd节点是几号节点：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph osd df tree</span><br></pre></td></tr></table></figure></p>
<p>可以获得以下信息：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ID CLASS WEIGHT  REWEIGHT SIZE   USE     AVAIL   %USE  VAR  PGS TYPE NAME      </span><br><span class="line">-1       0.03908        - 40 GiB 3.0 GiB  37 GiB  7.52 1.00   - root default   </span><br><span class="line">-3       0.01949        - 20 GiB 1.0 GiB  19 GiB  5.02 0.67   -     host node1 </span><br><span class="line"> 0   hdd 0.01949  1.00000 20 GiB 1.0 GiB  19 GiB  5.02 0.67   0         osd.0  </span><br><span class="line">-5       0.00980        - 10 GiB 1.0 GiB 9.0 GiB 10.03 1.33   -     host node2 </span><br><span class="line"> 1   hdd 0.00980  1.00000 10 GiB 1.0 GiB 9.0 GiB 10.03 1.33   0         osd.1  </span><br><span class="line">-7       0.00980        - 10 GiB 1.0 GiB 9.0 GiB 10.03 1.33   -     host node3 </span><br><span class="line"> 2   hdd 0.00980  1.00000 10 GiB 1.0 GiB 9.0 GiB 10.03 1.33   0         osd.2  </span><br><span class="line">                    TOTAL 40 GiB 3.0 GiB  37 GiB  7.52</span><br></pre></td></tr></table></figure></p>
<p>可以看到主机名和在其上创建的osd编号，我们此处将删除node3上的osd.2，删除操作我们在相应的节点机上执行。
&emsp;&emsp;我们先将需要删除的节点的权重置为0，然后将其踢出集群，接着停止该osd的服务，再然后从crush map中将其移除,再接着删除其的认证密钥，最后把其进行删除：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph osd crush reweight osd.2 0</span><br><span class="line">$ sudo ceph osd out 2</span><br><span class="line">$ sudo systemctl stop ceph-osd@2</span><br><span class="line">$ sudo ceph osd crush remove osd.2</span><br><span class="line">$ sudo ceph auth del osd.2</span><br><span class="line">$ sudo ceph osd rm 2</span><br></pre></td></tr></table></figure></p>
<h1 id="三-在搭建Ceph集群中碰到的一些情况"><a href="#三-在搭建Ceph集群中碰到的一些情况" class="headerlink" title="三. 在搭建Ceph集群中碰到的一些情况"></a>三. 在搭建Ceph集群中碰到的一些情况</h1><p>&emsp;&emsp;由于自己是新手，在搭建Ceph集群的过程中也碰到了较多的麻烦与问题，因此，在这一部分内容中，将对自己在这一过程中碰到的一些问题与情况做一个总结与梳理，也让更多的人在碰到这样的问题时能够作为解决方法的参考。</p>
<h2 id="1-时钟漂移问题："><a href="#1-时钟漂移问题：" class="headerlink" title="1. 时钟漂移问题："></a>1. 时钟漂移问题：</h2><p>&emsp;&emsp;由于Cpeh集群是由多个主机来一起构建的，而每个主机都可能会存在时间差，这就会导致集群中不同节点之间存在时间漂移。有时候我们在查看Ceph集群状态时就会发现这个问题：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">health: HEALTH_WARN</span><br><span class="line">            clock skew detected on mon.node2</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;因此这时候我们就需要依靠ntp服务来解决这个问题了，ntp全称为Network Time Protocol，ntp提供准确的网络时间源，我们打开该服务，并设置好网络时间源，且将所有节点的ntp都指向同一时间源即可解决时钟漂移问题。
&emsp;&emsp;首先给所有集群节点安装ntp服务包并打开服务且设置为开机启动：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ntpd -y</span><br><span class="line">$ sudo systemctl start ntpd</span><br><span class="line">$ sudo systemctl enable ntpd</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;然后修改配置文件，我们在其中一个节点上设置网络时间源，我在我自己的node1节点上进行设置：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/ntp.conf</span><br></pre></td></tr></table></figure></p>
<p>修改内容为：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line">server time1.aliyun.com minpoll 3 maxpoll 4 iburst</span><br><span class="line">server time2.aliyun.com minpoll 3 maxpoll 4 iburst</span><br><span class="line">server time3.aliyun.com minpoll 3 maxpoll 4 iburst</span><br></pre></td></tr></table></figure></p>
<p>将原有的server部分注释掉，并使用阿里云的时间源。而我们在其他节点上的相同的文件内修改的内容为：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line">server node1 minpoll 3 maxpoll 4 iburst</span><br></pre></td></tr></table></figure></p>
<p>同样也是将之前的server部分注释掉，但这里新增的语句是指向设置了网络时间源的节点，然后再事件源指向node1，最后重启一下ntp服务：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart ntpd</span><br></pre></td></tr></table></figure></p>
<p>可能在所有ntp服务设置好之后，时钟漂移问题还会存在，这需要等待一小段时间，让时钟真正同步后才OK。</p>
<h2 id="2-monotor节点根目录占用量太高："><a href="#2-monotor节点根目录占用量太高：" class="headerlink" title="2. monotor节点根目录占用量太高："></a>2. monotor节点根目录占用量太高：</h2><p>&emsp;&emsp;有时候我们会看到这样一个Ceph集群状态健康警报：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">health: HEALTH_WARN</span><br><span class="line">            mon node1 is low on available space monitor</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;该问题并不是由于monitor所在节点的osd的可用容量不足而是由于monitor所在的节点的根目录使用率太高了，剩余空间较小，因此解决方法是对根目录进行扩容，这样即可解决问题。</p>
<h2 id="3-osd-usage为0："><a href="#3-osd-usage为0：" class="headerlink" title="3. osd usage为0："></a>3. osd usage为0：</h2><p>&emsp;&emsp;有时候我们发现会出现在以下情况，集群状态里提示我们没有激活的mgr，此时的data字段中，所有数字都显示为0，这正是由于我们没有激活mgr所导致的。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cluster:</span><br><span class="line">  id:     85313a2d-104f-4425-a033-7f17888f8021</span><br><span class="line">  health: HEALTH_WARN</span><br><span class="line">              no active mgr</span><br><span class="line"> </span><br><span class="line">services:</span><br><span class="line">  mon: 2 daemons, quorum node3,node2</span><br><span class="line">  mgr: no active mgr</span><br><span class="line">  osd: 2 osds: 2 up, 2 in</span><br><span class="line"> </span><br><span class="line">data:</span><br><span class="line">  pools:   0 pools, 0 pgs</span><br><span class="line">  objects: 0  objects, 0 B</span><br><span class="line">  usage:   0 GiB used, 0 GiB / 0 GiB avail</span><br><span class="line">  pgs:</span><br></pre></td></tr></table></figure></p>
<p>针对此情况，我们激活mgr就好，我们在部署机上输入以下命令，此处我们选择在node1上激活mgr：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mgr create node1</span><br></pre></td></tr></table></figure></p>
<h2 id="4-将删除过的osd所在的lvm分区重新来创建osd："><a href="#4-将删除过的osd所在的lvm分区重新来创建osd：" class="headerlink" title="4. 将删除过的osd所在的lvm分区重新来创建osd："></a>4. 将删除过的osd所在的lvm分区重新来创建osd：</h2><p>&emsp;&emsp;如果我们想使用一个lvm分区来创建osd，在创建命令输入后报如下的错：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[node3][DEBUG ]  stderr: purged osd.1</span><br><span class="line">[node3][ERROR ] RuntimeError: command returned non-zero exit status: 1</span><br><span class="line">[ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data vg4/lv1</span><br><span class="line">[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs</span><br></pre></td></tr></table></figure></p>
<p>这是因为你所使用的这个lvm分区之前用来创建过osd，因此我们需要在重新创建osd之前做一个lvm分区的擦除操作：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph-volume lvm zap volumename/logicalname</span><br></pre></td></tr></table></figure></p>
<h1 id="四-末语"><a href="#四-末语" class="headerlink" title="四. 末语"></a>四. 末语</h1><p>&emsp;&emsp;以上都是自己在实验中一步一步尝试然后总结下来的，在整个搭建的过程中也是经历坎坷，网上虽然也有很多对应的教程，但都比较杂乱，有的教程给的命令甚至无法运行。以上的个人总结也非完美，可能还是会存在纰漏之处，若有问题欢迎交流讨论。</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>你们的支持是我源源不断创作的动力！！！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/uploads/wechatpay.JPG" alt="Ching 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/uploads/alipay.JPG" alt="Ching 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Ceph/" rel="tag"># Ceph</a>
          
            <a href="/tags/分布式存储/" rel="tag"># 分布式存储</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/20/GitHub-Hexo/" rel="next" title="使用GitHubPage+Hexo搭建个人博客教程">
                <i class="fa fa-chevron-left"></i> 使用GitHubPage+Hexo搭建个人博客教程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/01/VmwareNetWork/" rel="prev" title="虚拟机中的网络设置">
                虚拟机中的网络设置 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/IMG_2745.JPG" alt="Ching">
            
              <p class="site-author-name" itemprop="name">Ching</p>
              <p class="site-description motion-element" itemprop="description">努力便是晴天。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/ZzIris" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一-前言"><span class="nav-number">1.</span> <span class="nav-text">一. 前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二-Ceph集群搭建操作步骤"><span class="nav-number">2.</span> <span class="nav-text">二. Ceph集群搭建操作步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-前期准备工作"><span class="nav-number">2.1.</span> <span class="nav-text">1. 前期准备工作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装epel-release"><span class="nav-number">2.1.1.</span> <span class="nav-text">安装epel-release</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装Ceph相关的包"><span class="nav-number">2.1.2.</span> <span class="nav-text">安装Ceph相关的包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装openssh-server"><span class="nav-number">2.1.3.</span> <span class="nav-text">安装openssh-server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为部署机及每个集群节点机创建用户并进行sudo免密操作"><span class="nav-number">2.1.4.</span> <span class="nav-text">为部署机及每个集群节点机创建用户并进行sudo免密操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#设置部署机与其余集群节点的免密ssh连接"><span class="nav-number">2.1.5.</span> <span class="nav-text">设置部署机与其余集群节点的免密ssh连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关闭firewall服务和selinux服务"><span class="nav-number">2.1.6.</span> <span class="nav-text">关闭firewall服务和selinux服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装Ceph"><span class="nav-number">2.1.7.</span> <span class="nav-text">安装Ceph</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#方式一：通过外网镜像源安装"><span class="nav-number">2.1.7.1.</span> <span class="nav-text">方式一：通过外网镜像源安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方式二：通过自己搭建本地源进行安装"><span class="nav-number">2.1.7.2.</span> <span class="nav-text">方式二：通过自己搭建本地源进行安装</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-部署Ceph集群"><span class="nav-number">2.2.</span> <span class="nav-text">2. 部署Ceph集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#新建部署所用目录"><span class="nav-number">2.2.1.</span> <span class="nav-text">新建部署所用目录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建monitor"><span class="nav-number">2.2.2.</span> <span class="nav-text">创建monitor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分发命令权限"><span class="nav-number">2.2.3.</span> <span class="nav-text">分发命令权限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建mgr"><span class="nav-number">2.2.4.</span> <span class="nav-text">创建mgr</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建osd"><span class="nav-number">2.2.5.</span> <span class="nav-text">创建osd</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看集群部署情况"><span class="nav-number">2.2.6.</span> <span class="nav-text">查看集群部署情况</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-新增monitor节点："><span class="nav-number">2.3.</span> <span class="nav-text">3. 新增monitor节点：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-新增osd节点："><span class="nav-number">2.4.</span> <span class="nav-text">4. 新增osd节点：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-删除osd节点："><span class="nav-number">2.5.</span> <span class="nav-text">5. 删除osd节点：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三-在搭建Ceph集群中碰到的一些情况"><span class="nav-number">3.</span> <span class="nav-text">三. 在搭建Ceph集群中碰到的一些情况</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-时钟漂移问题："><span class="nav-number">3.1.</span> <span class="nav-text">1. 时钟漂移问题：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-monotor节点根目录占用量太高："><span class="nav-number">3.2.</span> <span class="nav-text">2. monotor节点根目录占用量太高：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-osd-usage为0："><span class="nav-number">3.3.</span> <span class="nav-text">3. osd usage为0：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-将删除过的osd所在的lvm分区重新来创建osd："><span class="nav-number">3.4.</span> <span class="nav-text">4. 将删除过的osd所在的lvm分区重新来创建osd：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#四-末语"><span class="nav-number">4.</span> <span class="nav-text">四. 末语</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ching</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
